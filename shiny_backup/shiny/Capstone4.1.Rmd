---
title: "Capstone4.1"
author: "Christopher Stewart"
date: "April 13, 2015"
output: html_document
---

## Introduction 
This document reports on the Capstone project marking the end of the 9-course Data Science Specialization offered by Coursera and the John Hopkins Department of Biostatistics. The purpose of this project is to apply the knowledge gained throughout the specialization's courses to a novel data science problem: text prediction. Specifically, we use large text files to build a text prediction algorithmthat is then incorporated into an interface that can be accessed by others. The project is offered in cooperation with Swiftkey, a company building smart prediction technology for easier mobile typing. Documentation on my Shiny data product is available in an [R Studio Presenter presentation](insert URL here).

I have elected to complete the project in R as per the parameters of the assignment, but also in Python to get hands-on experience with the Python's Natural Language Toolkit ([NLTK](http://www.nltk.org/)). A report on the Python version of the project is available [here](insert URL here).


## Data preparation and exploration

### Data Acquisition

Initially, we download the data and unzip the data files, switching working directories to that of the English language text files. We then print out the contents of the current directory to ensure that everything is in order.

```{r data acquisition}
suppressPackageStartupMessages(require("downloader")); suppressPackageStartupMessages(require("R.utils"))

# Download, unzip data and setwd()
url <- "http://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
download(url, dest = "data.zip", mode = "wb")
unzip("data.zip", exdir = "./")

# Set working directory
setwd(paste(getwd(),"/final/en_US",sep=""))
list.files()

# Clean up
rm(url)
```

### Test, Training and Corpus Split

Next we read in the data and divide the "blogs", "news" and "twitter" data into three parts:

1. Our _test_ set will be used as a final metric of the accuracy of the text prediction algorithm. For it, we set aside 10% of each data file.

2. From the remaining 90%, 10% is set aside as _training_ data, to be used to improve the model derived from the bulk of the data. 

3. The remaining data will be referred to as the _corpus_ data. This data will be used to develop the initial model. 

For now, both of the latter are grouped as a "rest" object.


```{r load and separate test from rest}
suppressPackageStartupMessages(require("stats"))

blogs <- readLines("en_US.blogs.txt")
news <- readLines("en_US.news.txt")
tweets <- readLines("en_US.twitter.txt", skipNul = TRUE)

set.seed(1)
blogs.test <- blogs[sample(1:length(blogs), 0.10*length(blogs), replace = FALSE)]
blogs.rest <- blogs[!blogs %in% blogs.test]

news.test <- news[sample(1:length(news), 0.10*length(news), replace = FALSE)]
news.rest <- news[!news %in% news.test]

tweets.test <- tweets[sample(1:length(tweets), 0.10*length(tweets), replace = FALSE)]
tweets.rest <- tweets[!tweets %in% tweets.test]

# Clean up
rm(blogs); rm(news); rm(tweets)
rm(blogs.test); rm(news.test); rm(tweets.test)
```


### Data cleaning

A quick look at the first few lines of each corpus reveal the presence of elements that we *don't want* to predict, including emoticons, numbers, profanity, punctuation, etc. We clean our corpora so that these do not go into the models. 


```{r clean data using regex}
suppressPackageStartupMessages(require("stringr"))

profanity_list <- readLines("http://www.bannedwordlist.com/lists/swearWords.txt", warn = FALSE)

# blogs
blogs.rest.1 <- tolower(blogs.rest)
blogs.rest.2 <- str_replace_all(blogs.rest.1, "[^[:alnum:][:space:]'|’]", ""); blogs.rest.2 <- iconv(blogs.rest.2, from="UTF-8", to="ascii", sub=""); blogs.rest.2 <- iconv(blogs.rest.2, to="ASCII//TRANSLIT")
blogs.rest.3 <- str_replace_all(blogs.rest.2, "[[:digit:]]+", "")
blogs.rest.4 <- str_replace_all(blogs.rest.3, paste(profanity_list, collapse = "|"), replacement = "")

### Clean up
rm(blogs.rest); rm(blogs.rest.1); rm(blogs.rest.2); rm(blogs.rest.3)

# news
news.rest.1 <- tolower(news.rest)
news.rest.2 <- str_replace_all(news.rest.1, "[^[:alnum:][:space:]'|’]", ""); news.rest.2 <- iconv(news.rest.2, from="UTF-8", to="ascii", sub=""); news.rest.2 <- iconv(news.rest.2, to="ASCII//TRANSLIT")
news.rest.3 <- str_replace_all(news.rest.2, "[[:digit:]]+", "")
news.rest.4 <- str_replace_all(news.rest.3, paste(profanity_list, collapse = "|"), replacement = "")

### Clean up
rm(news.rest); rm(news.rest.1); rm(news.rest.2); rm(news.rest.3)

# tweets
tweets.rest.1 <- tolower(tweets.rest)
tweets.rest.2 <- str_replace_all(tweets.rest.1, "[^[:alnum:][:space:]'|’]", ""); tweets.rest.2 <- iconv(tweets.rest.2, from="UTF-8", to="ascii", sub=""); tweets.rest.2 <- iconv(tweets.rest.2, to="ASCII//TRANSLIT")
tweets.rest.3 <- str_replace_all(tweets.rest.2, "[[:digit:]]+", "")
tweets.rest.4 <- str_replace_all(tweets.rest.3, paste(profanity_list, collapse = "|"), replacement = "")

## Clean up
rm(tweets.rest); rm(tweets.rest.1); rm(tweets.rest.2); rm(tweets.rest.3)

```


In the next step of data processing, we use R's [stringi](http://cran.r-project.org/web/packages/stringi/index.html) package to look at highly infrequent terms in the data. A quick look at the blogs data (below) reveals that almost half (46%) of the tokens only occur once. 

```{r}
suppressPackageStartupMessages(require("stringi"))
x_name <- "grams"; y_name <- "frequency"

blogs.rest.tok <- sort(table(unlist(strsplit(blogs.rest.4, split = "[[:space:]]+"))), decreasing=TRUE)
blogs.rest.tok_freqs <- as.numeric(unlist(regmatches(blogs.rest.tok, gregexpr("[[:digit:]]+", blogs.rest.tok))))
blogs.rest.tok_tab = data.frame(names(blogs.rest.tok), blogs.rest.tok_freqs); names(blogs.rest.tok_tab) <- c(x_name, y_name)

# Look at frequency of unigrams w/ n = 1
blogs_1grams.nof1 <- subset(blogs.rest.tok_tab, frequency <= 1)
cat('Out of', length(blogs.rest.tok_tab$grams), 'tokens, about', round((length(blogs_1grams.nof1$grams) / length(blogs.rest.tok_tab$grams) * 100), digits = 0), '% occur only once.')

# Clean up
rm(blogs.rest.tok); rm(blogs.rest.tok_freqs); rm(blogs.rest.tok_tab); rm(blogs_1grams.nof1)

```

This finding suggests a high degree of sparsity in the data, an issue that will have to be addressed during model building.


## Modeling

### Corpus and Validation Data

At this point, we separate the corpus data from the validation data. The latter will be used to refine our model, the former is used to construct a first pass at it. 

```{r separating corpus (80%) from validation (10%) data}

set.seed(2)
blogs.val <- blogs.rest.4[sample(1:length(blogs.rest.4), 0.10*length(blogs.rest.4), replace = FALSE)]; blogs.corpus <- blogs.rest.4[!blogs.rest.4 %in% blogs.val]

news.val <- news.rest.4[sample(1:length(news.rest.4), 0.10*length(news.rest.4), replace = FALSE)]; news.corpus <- news.rest.4[!news.rest.4 %in% news.val]


tweets.val <- tweets.rest.4[sample(1:length(tweets.rest.4), 0.10*length(tweets.rest.4), replace = FALSE)]; tweets.corpus <- tweets.rest.4[!tweets.rest.4 %in% tweets.val]

#Clean up
rm(blogs.rest.4); rm(news.rest.4); rm(tweets.rest.4)
```

### Tokenization, n-gram constructions, frequency tables and n-gram probabilities

We now tokenize and produce 2-, 3- and 4-grams using Maciej Szymkiewicz's efficient [Ngrams_tokenizer](https://github.com/zero323/r-snippets/blob/master/R/ngram_tokenizer.R) function. We write tokenized 

```{r tokenize}
dir.create(path = "./tokenized/")

source("Ngrams_tokenizer.R")
tokenizer <- ngram_tokenizer(1)
blogs.corpus_tok <- tokenizer(blogs.corpus)
news.corpus_tok <- tokenizer(news.corpus)
tweets.corpus_tok <- tokenizer(tweets.corpus)
write.table(blogs.corpus_tok, file = "./tokenized/blogs.corpus_tok.txt")
write.table(news.corpus_tok, file = "./tokenized/news.corpus_tok.txt")
write.table(tweets.corpus_tok, file = "./tokenized/tweets.corpus_tok.txt")
```

Next, we process our tokens in order to produce higher order n-grams with MLEs (called "p" in data files). Sparsity is addressed by eliminating highly infrequent data.

```{r unigrams}
suppressPackageStartupMessages(require("data.table"))
setwd(paste(getwd(),"/tokenized",sep=""))

## Blogs
blogs.corpus_tok <- fread("blogs.corpus_tok.txt"); blogs.corpus_tok[,c(1,2,4) := NULL]; setnames(blogs.corpus_tok, "V3", "token")
blogs.tokC <- blogs.corpus_tok[,token.count:=.N, by = token]
blogs.tokP <- blogs.tokC[,p := round((token.count / sum(unique(token.count))), digits = 5)]
blogs.tokP[, 2 := NULL]
blogs.tok_PUNK <- blogs.tokP[p < 0.00001, token := "UNK"]

# Get p-values, indices and words & indices; write to file
blogs.tok_ps <- subset(unique(blogs.tok_PUNK[,. (token, p)])); blogs.tok_ps <- as.matrix(blogs.tok_ps[, p])
save(blogs.tok_ps, file = "blogs.tok_ps.RData")

blogs.toks <- t(blogs.tok_PUNK[, token]); save(blogs.toks, file = "blogs.toks.RData")

# Clean up
rm(blogs.corpus_tok); rm(blogs.tokC); rm(blogs.tokP); rm(blogs.tok_PUNK); rm(blogs.tok_ps); rm(blogs.toks)


## News
news.corpus_tok <- fread("news.corpus_tok.txt"); news.corpus_tok[,c(1,2,4) := NULL]; setnames(news.corpus_tok, "V3", "token")
news.tokC <- news.corpus_tok[,token.count:=.N, by = token]
news.tokP <- news.tokC[,p := round((token.count / sum(unique(token.count))), digits = 5)]
news.tokP[, 2 := NULL]
news.tok_PUNK <- news.tokP[p < 0.00001, token := "UNK"]

# Get p-values, indices and words & indices; write to file
news.tok_ps <- subset(unique(news.tok_PUNK[,. (token, p)])); news.tok_ps <- as.matrix(news.tok_ps[, p])
save(news.tok_ps, file = "news.tok_ps.RData")

news.toks <- t(news.tok_PUNK[, token]); save(news.toks, file = "news.toks.RData")

# Clean up
rm(news.corpus_tok); rm(news.tokC); rm(news.tokP); rm(news.tok_PUNK); rm(news.tok_ps); rm(news.toks)


## Tweets
tweets.corpus_tok <- fread("tweets.corpus_tok.txt"); tweets.corpus_tok[,c(1,2,4) := NULL]; setnames(tweets.corpus_tok, "V3", "token")
tweets.tokC <- tweets.corpus_tok[,token.count:=.N, by = token]
tweets.tokP <- tweets.tokC[,p := round((token.count / sum(unique(token.count))), digits = 5)]
tweets.tokP[, 2 := NULL]
tweets.tok_PUNK <- tweets.tokP[p < 0.00001, token := "UNK"]

# Get p-values, indices and words & indices; write to file
tweets.tok_ps <- subset(unique(tweets.tok_PUNK[,. (token, p)])); tweets.tok_ps <- as.matrix(tweets.tok_ps[, p])
save(tweets.tok_ps, file = "tweets.tok_ps.RData")

tweets.toks <- t(tweets.tok_PUNK[, token]); save(tweets.toks, file = "tweets.toks.RData")

# Clean up
rm(tweets.corpus_tok); rm(tweets.tokC); rm(tweets.tokP); rm(tweets.tok_PUNK); rm(tweets.tok_ps); rm(tweets.toks)

```

We now make bigrams...

```{r bigrams}

## BLOGS
load("blogs.toks.RData")
blogs.toks1 <- noquote(as.vector(blogs.toks))

source("Ngrams_tokenizer.R")
bigram.tokenizer <- ngram_tokenizer(2)

## Make bigrams of sample, eliminate bigrams w/ "UNK", get counts, eliminate bigrams with C(W) <= 3
blogs.bi <- bigram.tokenizer(blogs.toks1); blogs.biDT <- as.data.table(blogs.bi); setnames(blogs.biDT, "bigram")
blogs.biDT1 <- blogs.biDT[!bigram %like% "UNK"]

blogs.biDT2 <- blogs.biDT1[,bigram.count:=.N, by = bigram]
blogs.biDT3 <- subset(blogs.biDT2, bigram.count > 3) 

# Eliminate duplication, split "gram" from "target" and get rid of bigrams
blogs.biDT4 <- blogs.biDT3[order(-bigram.count)]; blogs.biDT4.1 <- unique(blogs.biDT4)
blogs.biDT5 <- blogs.biDT4.1[, c("gram", "target") := tstrsplit(bigram, " ", fixed=TRUE)]
blogs.biDT5[,1:=NULL]
setcolorder(blogs.biDT5, c("gram", "target", "bigram.count"))

# Order by gram, add gram & "unigram" counts, calculate ps
blogs.biDT6 <- blogs.biDT5[order(gram)]
blogs.biDT6[,gram.count:=.N, by = gram]
blogs.biDT6[,unigram.count := gram.count + bigram.count]
blogs.biDT7 <- blogs.biDT6[,p := round((bigram.count / unigram.count), digits = 4)]
blogs.biDT7[,c(3,4,5):=NULL]

# Clean up
rm(blogs.bi); rm(blogs.biDT); rm(blogs.biDT1); rm(blogs.biDT2); rm(blogs.biDT3); rm(blogs.biDT4); rm(blogs.biDT4.1); rm(blogs.biDT5); rm(blogs.biDT6)


## NEWS
load("news.toks.RData")
news.toks1 <- noquote(as.vector(news.toks))

## Make bigrams of sample, eliminate bigrams w/ "UNK", get counts, eliminate bigrams with C(W) <= 3
news.bi <- bigram.tokenizer(news.toks1); news.biDT <- as.data.table(news.bi); setnames(news.biDT, "bigram")
news.biDT1 <- news.biDT[!bigram %like% "UNK"]

news.biDT2 <- news.biDT1[,bigram.count:=.N, by = bigram]
news.biDT3 <- subset(news.biDT2, bigram.count > 3) 

# Eliminate duplication, split "gram" from "target" and get rid of bigrams
news.biDT4 <- news.biDT3[order(-bigram.count)]; news.biDT4.1 <- unique(news.biDT4)
news.biDT5 <- news.biDT4.1[, c("gram", "target") := tstrsplit(bigram, " ", fixed=TRUE)]
news.biDT5[,1:=NULL]
setcolorder(news.biDT5, c("gram", "target", "bigram.count"))

# Order by gram, add gram & "unigram" counts, calculate ps
news.biDT6 <- news.biDT5[order(gram)]
news.biDT6[,gram.count:=.N, by = gram]
news.biDT6[,unigram.count := gram.count + bigram.count]
news.biDT7 <- news.biDT6[,p := round((bigram.count / unigram.count), digits = 4)]
news.biDT7[,c(3,4,5):=NULL]

# Clean up
rm(news.bi); rm(news.biDT); rm(news.biDT1); rm(news.biDT2); rm(news.biDT3); rm(news.biDT4); rm(news.biDT4.1); rm(news.biDT5); rm(news.biDT6)


## TWEETS
load("tweets.toks.RData")
tweets.toks1 <- noquote(as.vector(tweets.toks))

source("Ngrams_tokenizer.R")
bigram.tokenizer <- ngram_tokenizer(2)

## Make bigrams of sample, eliminate bigrams w/ "UNK", get counts, eliminate bigrams with C(W) <= 3
tweets.bi <- bigram.tokenizer(tweets.toks1); tweets.biDT <- as.data.table(tweets.bi); setnames(tweets.biDT, "bigram")
tweets.biDT1 <- tweets.biDT[!bigram %like% "UNK"]

tweets.biDT2 <- tweets.biDT1[,bigram.count:=.N, by = bigram]
tweets.biDT3 <- subset(tweets.biDT2, bigram.count > 3) 

# Eliminate duplication, split "gram" from "target" and get rid of bigrams
tweets.biDT4 <- tweets.biDT3[order(-bigram.count)]; tweets.biDT4.1 <- unique(tweets.biDT4)
tweets.biDT5 <- tweets.biDT4.1[, c("gram", "target") := tstrsplit(bigram, " ", fixed=TRUE)]
tweets.biDT5[,1:=NULL]
setcolorder(tweets.biDT5, c("gram", "target", "bigram.count"))

# Order by gram, add gram & "unigram" counts, calculate ps
tweets.biDT6 <- tweets.biDT5[order(gram)]
tweets.biDT6[,gram.count:=.N, by = gram]
tweets.biDT6[,unigram.count := gram.count + bigram.count]
tweets.biDT7 <- tweets.biDT6[,p := round((bigram.count / unigram.count), digits = 4)]
tweets.biDT7[,c(3,4,5):=NULL]

# Clean up
rm(tweets.bi); rm(tweets.biDT); rm(tweets.biDT1); rm(tweets.biDT2); rm(tweets.biDT3); rm(tweets.biDT4); rm(tweets.biDT4.1); rm(tweets.biDT5); rm(tweets.biDT6)

```

trigrams...

```{r trigrams}
trigram.tokenizer <- ngram_tokenizer(3)

## BLOGS
## Make trigrams of sample, eliminate trigrams w/ "UNK", get counts, eliminate trigrams with C(W) <= 3
blogs.tri <- trigram.tokenizer(blogs.toks1); blogs.triDT <- as.data.table(blogs.tri); setnames(blogs.triDT, "trigram")
blogs.triDT1 <- blogs.triDT[!trigram %like% "UNK"]

blogs.triDT2 <- blogs.triDT1[,trigram.count:=.N, by = trigram]
blogs.triDT3 <- subset(blogs.triDT2, trigram.count > 3) 

# Eliminate duplication, split "wi_2", "gram" and "target" and get rid of trigrams
blogs.triDT4 <- blogs.triDT3[order(-trigram.count)]; blogs.trisampDT4.1 <- unique(blogs.trisampDT4)
blogs.triDT5 <- blogs.triDT4.1[, c("wi_2", "gram", "target") := tstrsplit(trigram, " ", fixed=TRUE)]
blogs.triDT5[,1:=NULL]
setcolorder(blogs.triDT5, c("wi_2", "gram", "target", "trigram.count"))

# Concatenate "wi_2" and "gram" into "bigram", then sort on "bigram"
blogs.triDT5.1 <- blogs.triDT5[, bigram := paste(blogs.triDT5[,wi_2], blogs.triDT5[,gram], sep = " ")]
blogs.triDT5.1[,c(1,2):=NULL]; setcolorder(blogs.triDT5.1, c("bigram", "target", "trigram.count"))

# Count "bigrams", count "twograms", get ps
blogs.triDT5.1 <- blogs.triDT5.1[order(bigram)]
blogs.triDT6 <- blogs.triDT5.1[,bigram.count:=.N, by = bigram]
blogs.triDT6.1 <- blogs.triDT6[,twogram.count := trigram.count + bigram.count]
blogs.triDT7 <- blogs.triDT6.1[,p := round((trigram.count / twogram.count), digits = 4)]

blogs.triDT7[,c(3,4,5):=NULL]

# Clean up
rm(blogs.toks); rm(blogs.toksamp); rm(blogs.trisamp); rm(blogs.trisampDT); rm(blogs.trisampDT1); rm(blogs.trisampDT2); rm(blogs.trisampDT3); rm(blogs.trisampDT4); rm(blogs.trisampDT4.1); rm(blogs.trisampDT5); rm(blogs.trisampDT5.1); rm(blogs.trisampDT6); rm(blogs.trisampDT6.1)


## NEWS
## Make trigrams of sample, eliminate trigrams w/ "UNK", get counts, eliminate trigrams with C(W) <= 3
news.tri <- trigram.tokenizer(news.toks1); news.triDT <- as.data.table(news.tri); setnames(news.triDT, "trigram")
news.triDT1 <- news.triDT[!trigram %like% "UNK"]

news.triDT2 <- news.triDT1[,trigram.count:=.N, by = trigram]
news.triDT3 <- subset(news.triDT2, trigram.count > 3) 

# Eliminate duplication, split "wi_2", "gram" and "target" and get rid of trigrams
news.triDT4 <- news.triDT3[order(-trigram.count)]; news.trisampDT4.1 <- unique(news.trisampDT4)
news.triDT5 <- news.triDT4.1[, c("wi_2", "gram", "target") := tstrsplit(trigram, " ", fixed=TRUE)]
news.triDT5[,1:=NULL]
setcolorder(news.triDT5, c("wi_2", "gram", "target", "trigram.count"))

# Concatenate "wi_2" and "gram" into "bigram", then sort on "bigram"
news.triDT5.1 <- news.triDT5[, bigram := paste(news.triDT5[,wi_2], news.triDT5[,gram], sep = " ")]
news.triDT5.1[,c(1,2):=NULL]; setcolorder(news.triDT5.1, c("bigram", "target", "trigram.count"))

# Count "bigrams", count "twograms", get ps
news.triDT5.1 <- news.triDT5.1[order(bigram)]
news.triDT6 <- news.triDT5.1[,bigram.count:=.N, by = bigram]
news.triDT6.1 <- news.triDT6[,twogram.count := trigram.count + bigram.count]
news.triDT7 <- news.triDT6.1[,p := round((trigram.count / twogram.count), digits = 4)]

news.triDT7[,c(3,4,5):=NULL]

# Clean up
rm(news.toks); rm(news.toksamp); rm(news.trisamp); rm(news.trisampDT); rm(news.trisampDT1); rm(news.trisampDT2); rm(news.trisampDT3); rm(news.trisampDT4); rm(news.trisampDT4.1); rm(news.trisampDT5); rm(news.trisampDT5.1); rm(news.trisampDT6); rm(news.trisampDT6.1)


## TWEETS
## Make trigrams of sample, eliminate trigrams w/ "UNK", get counts, eliminate trigrams with C(W) <= 3
tweets.tri <- trigram.tokenizer(tweets.toks1); tweets.triDT <- as.data.table(tweets.tri); setnames(tweets.triDT, "trigram")
tweets.triDT1 <- tweets.triDT[!trigram %like% "UNK"]

tweets.triDT2 <- tweets.triDT1[,trigram.count:=.N, by = trigram]
tweets.triDT3 <- subset(tweets.triDT2, trigram.count > 3) 

# Eliminate duplication, split "wi_2", "gram" and "target" and get rid of trigrams
tweets.triDT4 <- tweets.triDT3[order(-trigram.count)]; tweets.trisampDT4.1 <- unique(tweets.trisampDT4)
tweets.triDT5 <- tweets.triDT4.1[, c("wi_2", "gram", "target") := tstrsplit(trigram, " ", fixed=TRUE)]
tweets.triDT5[,1:=NULL]
setcolorder(tweets.triDT5, c("wi_2", "gram", "target", "trigram.count"))

# Concatenate "wi_2" and "gram" into "bigram", then sort on "bigram"
tweets.triDT5.1 <- tweets.triDT5[, bigram := paste(tweets.triDT5[,wi_2], tweets.triDT5[,gram], sep = " ")]
tweets.triDT5.1[,c(1,2):=NULL]; setcolorder(tweets.triDT5.1, c("bigram", "target", "trigram.count"))

# Count "bigrams", count "twograms", get ps
tweets.triDT5.1 <- tweets.triDT5.1[order(bigram)]
tweets.triDT6 <- tweets.triDT5.1[,bigram.count:=.N, by = bigram]
tweets.triDT6.1 <- tweets.triDT6[,twogram.count := trigram.count + bigram.count]
tweets.triDT7 <- tweets.triDT6.1[,p := round((trigram.count / twogram.count), digits = 4)]

tweets.triDT7[,c(3,4,5):=NULL]

# Clean up
rm(tweets.toks); rm(tweets.toksamp); rm(tweets.trisamp); rm(tweets.trisampDT); rm(tweets.trisampDT1); rm(tweets.trisampDT2); rm(tweets.trisampDT3); rm(tweets.trisampDT4); rm(tweets.trisampDT4.1); rm(tweets.trisampDT5); rm(tweets.trisampDT5.1); rm(tweets.trisampDT6); rm(tweets.trisampDT6.1)

```

and tetragrams.

```{r tetragrams}
tetragram.tokenizer <- ngram_tokenizer(4)

```{r}
tetragram.tokenizer <- ngram_tokenizer(4)

## BLOGS
## Make tetragrams of sample, eliminate tetragrams w/ "UNK", get counts, eliminate tetragrams with C(W) <= 3
blogs.tetra <- tetragram.tokenizer(blogs.toks1); blogs.tetraDT <- as.data.table(blogs.tetra); setnames(blogs.tetraDT, "tetragram")
blogs.tetraDT1 <- blogs.tetraDT[!tetragram %like% "UNK"]

blogs.tetraDT2 <- blogs.tetraDT1[,tetragram.count:=.N, by = tetragram]
blogs.tetraDT3 <- subset(blogs.tetraDT2, tetragram.count > 3) 

# Eliminate duplication, split "wi_2", "gram" and "target" and get rid of tetragrams
blogs.tetraDT4 <- blogs.tetraDT3[order(-tetragram.count)]; blogs.tetraDT4.1 <- unique(blogs.tetraDT4)
blogs.tetraDT5 <- blogs.tetraDT4.1[, c("wi_3", "wi_2", "gram", "target") := tstrsplit(tetragram, " ", fixed=TRUE)]
blogs.tetraDT5[,1:=NULL]
setcolorder(blogs.tetraDT5, c("wi_3", "wi_2", "gram", "target", "tetragram.count"))

# Concatenate "wi_3", "wi_2" & "gram" into "trigram", then sort on "trigram"
blogs.tetraDT5.1 <- blogs.tetraDT5[, trigram := paste(blogs.tetrasampDT5[,wi_3], blogs.tetrasampDT5[,wi_2], blogs.tetrasampDT5[,gram], sep = " ")]
blogs.tetraDT5.1[,c(1,2,3):=NULL]; setcolorder(blogs.tetraDT5.1, c("trigram", "target", "tetragram.count"))

# Count "trigrams", count "threegrams", get ps
blogs.tetraDT5.1 <- blogs.tetraDT5.1[order(trigram)]
blogs.tetraDT6 <- blogs.tetraDT5.1[,trigram.count:=.N, by = trigram]
blogs.tetraDT6.1 <- blogs.tetraDT6[,threegram.count := tetragram.count + trigram.count]
blogs.tetraDT7 <- blogs.tetraDT6.1[,p := round((tetragram.count / threegram.count), digits = 4)]

blogs.tetraDT7[,c(3,4,5):=NULL]

# Clean up
rm(blogs.toks1);  rm(blogs.tetra); rm(blogs.tetraDT); rm(blogs.tetraDT1); rm(blogs.tetraDT2); rm(blogs.tetraDT3); rm(blogs.tetraDT4); rm(blogs.tetraDT4.1); rm(blogs.tetraDT5); rm(blogs.tetraDT5.1); rm(blogs.tetraDT6); rm(blogs.tetraDT6.1)

## NEWS
## Make tetragrams of sample, eliminate tetragrams w/ "UNK", get counts, eliminate tetragrams with C(W) <= 3
news.tetra <- tetragram.tokenizer(news.toks1); news.tetraDT <- as.data.table(news.tetra); setnames(news.tetraDT, "tetragram")
news.tetraDT1 <- news.tetraDT[!tetragram %like% "UNK"]

news.tetraDT2 <- news.tetraDT1[,tetragram.count:=.N, by = tetragram]
news.tetraDT3 <- subset(news.tetraDT2, tetragram.count > 3) 

# Eliminate duplication, split "wi_2", "gram" and "target" and get rid of tetragrams
news.tetraDT4 <- news.tetraDT3[order(-tetragram.count)]; news.tetraDT4.1 <- unique(news.tetraDT4)
news.tetraDT5 <- news.tetraDT4.1[, c("wi_3", "wi_2", "gram", "target") := tstrsplit(tetragram, " ", fixed=TRUE)]
news.tetraDT5[,1:=NULL]
setcolorder(news.tetraDT5, c("wi_3", "wi_2", "gram", "target", "tetragram.count"))

# Concatenate "wi_3", "wi_2" & "gram" into "trigram", then sort on "trigram"
news.tetraDT5.1 <- news.tetraDT5[, trigram := paste(news.tetrasampDT5[,wi_3], news.tetrasampDT5[,wi_2], news.tetrasampDT5[,gram], sep = " ")]
news.tetraDT5.1[,c(1,2,3):=NULL]; setcolorder(news.tetraDT5.1, c("trigram", "target", "tetragram.count"))

# Count "trigrams", count "threegrams", get ps
news.tetraDT5.1 <- news.tetraDT5.1[order(trigram)]
news.tetraDT6 <- news.tetraDT5.1[,trigram.count:=.N, by = trigram]
news.tetraDT6.1 <- news.tetraDT6[,threegram.count := tetragram.count + trigram.count]
news.tetraDT7 <- news.tetraDT6.1[,p := round((tetragram.count / threegram.count), digits = 4)]

news.tetraDT7[,c(3,4,5):=NULL]

# Clean up
rm(news.toks1);  rm(news.tetra); rm(news.tetraDT); rm(news.tetraDT1); rm(news.tetraDT2); rm(news.tetraDT3); rm(news.tetraDT4); rm(news.tetraDT4.1); rm(news.tetraDT5); rm(news.tetraDT5.1); rm(news.tetraDT6); rm(news.tetraDT6.1)

## TWEETS 
## Make tetragrams of sample, eliminate tetragrams w/ "UNK", get counts, eliminate tetragrams with C(W) <= 3
tweets.tetra <- tetragram.tokenizer(tweets.toks1); tweets.tetraDT <- as.data.table(tweets.tetra); setnames(tweets.tetraDT, "tetragram")
tweets.tetraDT1 <- tweets.tetraDT[!tetragram %like% "UNK"]

tweets.tetraDT2 <- tweets.tetraDT1[,tetragram.count:=.N, by = tetragram]
tweets.tetraDT3 <- subset(tweets.tetraDT2, tetragram.count > 3) 

# Eliminate duplication, split "wi_2", "gram" and "target" and get rid of tetragrams
tweets.tetraDT4 <- tweets.tetraDT3[order(-tetragram.count)]; tweets.tetraDT4.1 <- unique(tweets.tetraDT4)
tweets.tetraDT5 <- tweets.tetraDT4.1[, c("wi_3", "wi_2", "gram", "target") := tstrsplit(tetragram, " ", fixed=TRUE)]
tweets.tetraDT5[,1:=NULL]
setcolorder(tweets.tetraDT5, c("wi_3", "wi_2", "gram", "target", "tetragram.count"))

# Concatenate "wi_3", "wi_2" & "gram" into "trigram", then sort on "trigram"
tweets.tetraDT5.1 <- tweets.tetraDT5[, trigram := paste(tweets.tetrasampDT5[,wi_3], tweets.tetrasampDT5[,wi_2], tweets.tetrasampDT5[,gram], sep = " ")]
tweets.tetraDT5.1[,c(1,2,3):=NULL]; setcolorder(tweets.tetraDT5.1, c("trigram", "target", "tetragram.count"))

# Count "trigrams", count "threegrams", get ps
tweets.tetraDT5.1 <- tweets.tetraDT5.1[order(trigram)]
tweets.tetraDT6 <- tweets.tetraDT5.1[,trigram.count:=.N, by = trigram]
tweets.tetraDT6.1 <- tweets.tetraDT6[,threegram.count := tetragram.count + trigram.count]
tweets.tetraDT7 <- tweets.tetraDT6.1[,p := round((tetragram.count / threegram.count), digits = 4)]

tweets.tetraDT7[,c(3,4,5):=NULL]

# Clean up
rm(tweets.toks1);  rm(tweets.tetra); rm(tweets.tetraDT); rm(tweets.tetraDT1); rm(tweets.tetraDT2); rm(tweets.tetraDT3); rm(tweets.tetraDT4); rm(tweets.tetraDT4.1); rm(tweets.tetraDT5); rm(tweets.tetraDT5.1); rm(tweets.tetraDT6); rm(tweets.tetraDT6.1)
rm(bigram.tokenizer); rm(trigram.tokenizer); rm(tetragram.tokenizer); rm(ngram_tokenizer)

```

# Modeling and Prediction

Having derived our n-grams, we start to build a model for text prediction. In this case, we will combine a backoff approach with a linear interpolation model (one more sentence about why this kind of model, then give a reference).

To begin with, we test out our approach on strings derived from the course's quizzes. 


```{r}

### This will have come in from Shiny app
test.string <- "Very early observations on the Bills game: Offense still struggling but the"

### Assume user has selected "blogs", "news" or "twitter"
corpus.type <- "tweets"


# Clean test.string using same steps as w/ the corpus
profanity_list <- readLines("http://www.bannedwordlist.com/lists/swearWords.txt", warn = FALSE)

test.string.1 <-tolower(test.string); test.string.2 <- str_replace_all(test.string.1, "[^[:alnum:][:space:]'|’]", ""); test.string.3 <- iconv(test.string.2, from="UTF-8", to="ascii", sub=""); test.string.4 <- iconv(test.string.3, to="ASCII//TRANSLIT"); test.string.5 <- str_replace_all(test.string.4, "[[:digit:]]+", ""); test.string.6 <- str_replace_all(test.string.5, paste(profanity_list, collapse = "|"), replacement = "")
test.string.7 <- strsplit(test.string.6, " ")


# Clean up
rm(test.string); rm(test.string.1); rm(test.string.2); rm(test.string.3); rm(test.string.4); rm(test.string.5); rm(test.string.6)

# Create ngrams for lookup
test.string_gram4 <- sapply(test.string.7, tail, 3); test.string_gram4.gram <- test.string_gram4[1:3, ]; test.string_gram4.gram <- as.character(paste(test.string_gram4.gram, collapse = " "))

test.string_gram3 <- sapply(test.string.7, tail, 2); test.string_gram3.gram <- test.string_gram3[1:2, ]; test.string_gram3.gram <- as.character(paste(test.string_gram3.gram, collapse = " "))

test.string_gram2 <- sapply(test.string.7, tail, 1)

## Look-up in blogs
setkey(blogs.tetraDT7); blogs.tetra_test <- blogs.tetraDT7[list(test.string_gram4.gram)][1]; blogs.tetra_test$p[is.na(blogs.tetra_test$p)] <- 0
setkey(blogs.triDT7); blogs.tri_test <- blogs.triDT7[list(test.string_gram3.gram)][1]; blogs.tri_test$p[is.na(blogs.tri_test$p)] <- 0
setkey(blogs.biDT7); blogs.bi_test <- blogs.biDT7[list(test.string_gram2)][1]; blogs.bi_test$p[is.na(blogs.bi_test$p)] <- 0

blogs.tetra_lambda <- (1/3)
blogs.tri_lambda <- (1/3)
blogs.bi_lambda <- (1/3)

blogs_ps <- c((blogs.tetra_test$p * blogs.tetra_lambda), (blogs.tri_test$p * blogs.tri_lambda), (blogs.bi_test$p * blogs.bi_lambda))
blogs_targets <- c(blogs.tetra_test$target, blogs.tri_test$target, blogs.bi_test$target)

blogs_targetDF <- data.frame(blogs_ps, blogs_targets); blogs_target <- as.character(with(blogs_targetDF, blogs_targets[blogs_ps== max(blogs_ps)]))

print(blogs_target)



## Look-up in news
setkey(news.tetraDT7); news.tetra_test <- news.tetraDT7[list(test.string_gram4.gram)][1]; news.tetra_test$p[is.na(news.tetra_test$p)] <- 0
setkey(news.triDT7); news.tri_test <- news.triDT7[list(test.string_gram3.gram)][1]; news.tri_test$p[is.na(news.tri_test$p)] <- 0
setkey(news.biDT7); news.bi_test <- news.biDT7[list(test.string_gram2)][1]; news.bi_test$p[is.na(news.bi_test$p)] <- 0

news.tetra_lambda <- (1/3)
news.tri_lambda <- (1/3)
news.bi_lambda <- (1/3)

news_ps <- c((news.tetra_test$p * news.tetra_lambda), (news.tri_test$p * news.tri_lambda), (news.bi_test$p * news.bi_lambda))
news_targets <- c(news.tetra_test$target, news.tri_test$target, news.bi_test$target)

news_targetDF <- data.frame(news_ps, news_targets); news_target <- as.character(with(news_targetDF, news_targets[news_ps== max(news_ps)]))

print(news_target)



## Look-up in tweets
setkey(tweets.tetraDT7); tweets.tetra_test <- tweets.tetraDT7[list(test.string_gram4.gram)][1]; tweets.tetra_test$p[is.na(tweets.tetra_test$p)] <- 0
setkey(tweets.triDT7); tweets.tri_test <- tweets.triDT7[list(test.string_gram3.gram)][1]; tweets.tri_test$p[is.na(tweets.tri_test$p)] <- 0
setkey(tweets.biDT7); tweets.bi_test <- tweets.biDT7[list(test.string_gram2)][1]; tweets.bi_test$p[is.na(tweets.bi_test$p)] <- 0

tweets.tetra_lambda <- (1/3)
tweets.tri_lambda <- (1/3)
tweets.bi_lambda <- (1/3)

tweets_ps <- c((tweets.tetra_test$p * tweets.tetra_lambda), (tweets.tri_test$p * tweets.tri_lambda), (tweets.bi_test$p * tweets.bi_lambda))
tweets_targets <- c(tweets.tetra_test$target, tweets.tri_test$target, tweets.bi_test$target)

tweets_targetDF <- data.frame(tweets_ps, tweets_targets); tweets_target <- as.character(with(tweets_targetDF, tweets_targets[tweets_ps== max(tweets_ps)]))

print(tweets_target)

```


The correct answers for the first three questions ("beer", "world", "happiest") are given by the Twitter model. Satisfied that this is a decent first go at the model, we move on to our Shiny app.




